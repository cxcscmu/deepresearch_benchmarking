World War II significantly transformed America by ending unemployment and boosting the economy as the nation emerged as the world's dominant economic power. The war effort led to the hiring of women, teenagers, the aged, and minorities, who were previously excluded from the workforce. Additionally, the United States rose from a midlevel global power to the leader of the "free world," marking a rapid increase in its global influence.