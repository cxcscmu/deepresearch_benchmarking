Yes, history should be taught in schools. It is considered an essential subject that provides students with a sense of identity, empathy, and understanding of the past, which helps them become informed and active citizens. Teaching history also allows students to see the past on its own terms and understand the complexities of historical events and movements.