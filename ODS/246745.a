Black history should be taught in schools because it recognizes African American contributions to American and world history, helps instill empathy and understanding, builds students' confidence, and empowers them to take control of their lives. Additionally, it fosters cultural understanding and validates the experiences of minoritized populations, promoting a sense of pride and belonging.