## Benchmarking deepresearch systems


### Setup

[TODO] detailed environment.

- overall you just need the openai package, ensure you have the latest version.
- for citation quality you also need to install crawl4ai (if not running that one, do not bother).


Create keys.env file with:  
OPENAI_API_KEY=...

### Origanization:

Main branch contains evaluation scripts.

Create branches to run specific DeepResearch frameworks on our queries. 

### Data

The queries we use to generate deepsearch reports are under:
- `./queries/researchy_queries_sample_doc_click.jsonl`
- format: {"id": \<id\>, "query": \<text\>}


The evaluation scripts read data from:

- `/data/group_data/cx_group/deepsearch_benchmark/reports/`
- This folder contains subfolders for each system, e.g:
    - GPTResearcher: answers generated by GPTResearcher with the original API
    - GPTResearcher_custom: answers generated by GPTResearcher with the original API with our API
    - The subfolders content follow this structure:
        - \<id\>.a : answer from deepresearch system for query with id \<id\>
        - \<id\>.q : query with id \<id\>


### Evaluation:

eval_citation_async.py : computes citation precision as per TREC-RAG

- `python eval_citation_async.py --subfolder [deepsearch_model_to_eval] --open_ai_model [llm_judge]`
    - [deepsearch_model_to_eval] must have a folder under `/data/group_data/cx_group/deepsearch_benchmark/reports`
    - [llm_judge] can be any model that supports OpenAI API (currently using gpt-4.1-mini)
    - writes `evaluation_results_citation_gpt-4.1-mini.json` to [deepsearch_model_to_eval] folder.
    - WARNING: this one can be slow and cost a lot of $. don't run unless you really need to.



eval_quality_async.py : computes holistic report quality using standard LLM as a judge viewpoints

- `python eval_citation_async.py --subfolder [deepsearch_model_to_eval] --open_ai_model [llm_judge]`
    - [deepsearch_model_to_eval] must have a folder under `/data/group_data/cx_group/deepsearch_benchmark/reports`
    - [llm_judge] can be any model that supports OpenAI API (currently using gpt-4.1-mini)
    - writes `evaluation_results_detailed_gpt-4.1-mini.json` to [deepsearch_model_to_eval] folder.
    - [TODO]: make prompts more strict.

- `python eval_merge.py --subfolder [deepsearch_model_to_eval] --open_ai_model [llm_judge]`
    - merges the two above json results.
    - `evaluation_results_per_query_score_gpt-4.1-mini.json` : per query scores
    - `evaluation_results_single_score_gpt-4.1-mini.json` : avg score over all queries




### Other / Utils

./systems contains baseline deepresearch systems:

- perplexity, through sonar deepresearch API
- openai, through gpt4o-search-preview [no better model is available through API]

./plots contains scripts to generate plots (WIP)

get_samples.py : pretty-print samples (query, report, and evaluation) to share with others (e.g., google drive)

